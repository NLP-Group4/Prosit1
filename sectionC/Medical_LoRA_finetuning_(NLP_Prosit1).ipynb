{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "yZjYgoalTHwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")"
      ],
      "metadata": {
        "id": "acvIpLD_TGkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "QvMRZ53eTEri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
        "model.to(device)\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "fvpGrtWTTCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize texts for language modeling\"\"\"\n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(\n",
        "        examples['medical_abstract'],\n",
        "        truncation=True,\n",
        "        max_length=512,  # context length\n",
        "        padding='max_length',\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    # For language modeling, labels are the same as input_ids\n",
        "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Load the full training data\n",
        "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
        "df_train = pd.read_parquet(\"hf://datasets/TimSchopf/medical_abstracts/\" + splits[\"train\"])\n",
        "df_test = pd.read_parquet(\"hf://datasets/TimSchopf/medical_abstracts/\" + splits[\"test\"])\n",
        "\n",
        "# Split training data into train and validation\n",
        "train_df, val_df = train_test_split(df_train, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(df_test)\n",
        "\n",
        "# Set pad token for tokenizer (DistilGPT-2 tokenizer doesn't have one by default)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "# Tokenize\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['condition_label', 'medical_abstract'])\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['condition_label', 'medical_abstract'])\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=['condition_label', 'medical_abstract'])\n",
        "\n",
        "print(\"Tokenization complete!\")"
      ],
      "metadata": {
        "id": "cC0N49cISx0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_token_accuracy(model, tokenizer, test_dataset, n_samples=100):\n",
        "    \"\"\"\n",
        "    How often is the correct next token in top-k predictions?\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    top1_correct = 0\n",
        "    top5_correct = 0\n",
        "    top10_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(n_samples, len(test_dataset))):\n",
        "            example = test_dataset[i]\n",
        "            input_ids = torch.tensor([example['input_ids']]).to(\"cuda\")\n",
        "\n",
        "            # Get predictions for each position\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Check each token (except first and last)\n",
        "            for pos in range(1, len(input_ids[0]) - 1):\n",
        "                if input_ids[0, pos] == tokenizer.pad_token_id:\n",
        "                    continue\n",
        "\n",
        "                true_token = input_ids[0, pos + 1].item()\n",
        "                predicted_logits = logits[0, pos, :]\n",
        "\n",
        "                # Top-k predictions\n",
        "                top_k = torch.topk(predicted_logits, k=10)\n",
        "                top_tokens = top_k.indices.tolist()\n",
        "\n",
        "                if true_token == top_tokens[0]:\n",
        "                    top1_correct += 1\n",
        "                if true_token in top_tokens[:5]:\n",
        "                    top5_correct += 1\n",
        "                if true_token in top_tokens[:10]:\n",
        "                    top10_correct += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "    return {\n",
        "        'top1_accuracy': top1_correct / total,\n",
        "        'top5_accuracy': top5_correct / total,\n",
        "        'top10_accuracy': top10_correct / total\n",
        "    }"
      ],
      "metadata": {
        "id": "yT6McwC7YEqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(model, dataset, tokenizer):\n",
        "    \"\"\"Calculate perplexity on a dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(100, len(dataset))):  # Sample 100 examples\n",
        "            example = dataset[i]\n",
        "            inputs = {\n",
        "                'input_ids': torch.tensor([example['input_ids']]).to(device),\n",
        "                'attention_mask': torch.tensor([example['attention_mask']]).to(device),\n",
        "                'labels': torch.tensor([example['labels']]).to(device)\n",
        "            }\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Count actual tokens (not padding)\n",
        "            n_tokens = inputs['attention_mask'].sum().item()\n",
        "\n",
        "            total_loss += loss.item() * n_tokens\n",
        "            total_tokens += n_tokens\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "rUB7l1h4YPRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Evaluating baseline (pretrained) model...\")\n",
        "baseline_perplexity = calculate_perplexity(model, test_dataset, tokenizer)\n",
        "print(f\"Baseline perplexity: {baseline_perplexity:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BASELINE GENERATION (Before Fine-tuning)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "prompt = \"The patient presented with\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=50,\n",
        "    num_return_sequences=3,\n",
        "    temperature=0.9,\n",
        "    do_sample=True,\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "for i, output in enumerate(outputs):\n",
        "    text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    print(f\"\\nSample {i+1}: {text}\")\n",
        "\n",
        "baseline_acc = next_token_accuracy(model, tokenizer, test_dataset)\n",
        "print(f\"{'Baseline':<15} {baseline_acc['top1_accuracy']:<10.1%} {baseline_acc['top5_accuracy']:<10.1%} {baseline_acc['top10_accuracy']:.1%}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NMx28oceaLEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # GPT-2 uses causal LM, not masked LM\n",
        ")"
      ],
      "metadata": {
        "id": "agyOlltVUVqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i1osfWySrMS"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,                        # LoRA rank\n",
        "    lora_alpha=32,              # LoRA scaling\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"]  # Apply to attention layers\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "# Should show: trainable params: ~300K / 124M (0.24%)\n",
        "\n",
        "# Move to GPU\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "# Training arguments (can use higher batch size now!)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-lora-finetuned\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,      # Can use 4 instead of 1!\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,                 # LoRA can use higher LR\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Training with LoRA...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save LoRA adapters (only ~2MB instead of 500MB!)\n",
        "model.save_pretrained(\"./gpt2-lora-final\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "# Extract training history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Separate train and eval logs\n",
        "train_logs = [x for x in log_history if 'loss' in x and 'eval_loss' not in x]\n",
        "eval_logs = [x for x in log_history if 'eval_loss' in x]\n",
        "\n",
        "train_steps = [x['step'] for x in train_logs]\n",
        "train_losses = [x['loss'] for x in train_logs]\n",
        "\n",
        "eval_steps = [x['step'] for x in eval_logs]\n",
        "eval_losses = [x['eval_loss'] for x in eval_logs]\n",
        "\n",
        "# Create visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curves\n",
        "ax1.plot(train_steps, train_losses, label='Train Loss', alpha=0.7)\n",
        "ax1.plot(eval_steps, eval_losses, label='Validation Loss', marker='o', linewidth=2)\n",
        "ax1.set_xlabel('Training Steps')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training Progress: Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Perplexity (exp(loss))\n",
        "train_perplexities = [math.exp(loss) for loss in train_losses]\n",
        "eval_perplexities = [math.exp(loss) for loss in eval_losses]\n",
        "\n",
        "ax2.plot(train_steps, train_perplexities, label='Train Perplexity', alpha=0.7)\n",
        "ax2.plot(eval_steps, eval_perplexities, label='Validation Perplexity', marker='o', linewidth=2)\n",
        "ax2.set_xlabel('Training Steps')\n",
        "ax2.set_ylabel('Perplexity')\n",
        "ax2.set_title('Training Progress: Perplexity')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LEARNING EVIDENCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Initial train loss: {train_losses[0]:.4f}\")\n",
        "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Improvement: {train_losses[0] - train_losses[-1]:.4f}\")\n",
        "print()\n",
        "print(f\"Initial val perplexity: {eval_perplexities[0]:.2f}\")\n",
        "print(f\"Final val perplexity: {eval_perplexities[-1]:.2f}\")\n",
        "print(f\"Improvement: {eval_perplexities[0] - eval_perplexities[-1]:.2f}\")"
      ],
      "metadata": {
        "id": "3qEbUNt5Tr1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Calculate perplexity on test set\n",
        "finetuned_perplexity = calculate_perplexity(model, test_dataset, tokenizer)\n",
        "\n",
        "print(f\"\\nPerplexity Comparison:\")\n",
        "print(f\"  Baseline (pretrained):  {baseline_perplexity:.2f}\")\n",
        "print(f\"  Fine-tuned:             {finetuned_perplexity:.2f}\")\n",
        "print(f\"  Improvement:            {baseline_perplexity - finetuned_perplexity:.2f}\")\n",
        "print(f\"  Relative improvement:   {(1 - finetuned_perplexity/baseline_perplexity)*100:.1f}%\")"
      ],
      "metadata": {
        "id": "QwiiKNXFVIh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_comparison(prompt, model, tokenizer, num_samples=3):\n",
        "    \"\"\"Generate text with the model\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        num_return_sequences=num_samples,\n",
        "        temperature=0.9,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    for output in outputs:\n",
        "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        results.append(text)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test prompts (adjust for your domain)\n",
        "test_prompts = [\n",
        "    \"The patient presented with\",\n",
        "    \"Treatment options include\",\n",
        "    \"The study found that\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATION COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PROMPT: '{prompt}'\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Fine-tuned generations\n",
        "    print(\"\\nFINE-TUNED MODEL:\")\n",
        "    print(\"-\" * 60)\n",
        "    finetuned_gens = generate_comparison(prompt, model, tokenizer, num_samples=3)\n",
        "    for i, text in enumerate(finetuned_gens, 1):\n",
        "        print(f\"\\n{i}. {text}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8JMMC3xbVchO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NEXT-TOKEN PREDICTION ACCURACY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Model':<15} {'Top-1':<10} {'Top-5':<10} {'Top-10'}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'Baseline':<15} {baseline_acc['top1_accuracy']:<10.1%} {baseline_acc['top5_accuracy']:<10.1%} {baseline_acc['top10_accuracy']:.1%}\")\n",
        "\n",
        "# Calculate accuracy for the fine-tuned model\n",
        "finetuned_acc = next_token_accuracy(model, tokenizer, test_dataset)\n",
        "print(f\"{'Fine-tuned':<15} {finetuned_acc['top1_accuracy']:<10.1%} {finetuned_acc['top5_accuracy']:<10.1%} {finetuned_acc['top10_accuracy']:.1%}\")"
      ],
      "metadata": {
        "id": "HdODxMEYVdGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3-xLpz5XXb1a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}