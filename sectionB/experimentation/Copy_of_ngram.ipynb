{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw4LMTaqjF6p"
      },
      "source": [
        "# Section B: Base Model (no smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9-ovcgcOZmt"
      },
      "outputs": [],
      "source": [
        "class NgramModel:\n",
        "    def __init__(self, n=2):\n",
        "        self.n = n\n",
        "        self.ngram_counts = {}\n",
        "        self.context_counts = {}\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        import re\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        words = text.split()\n",
        "        return words\n",
        "\n",
        "    def train(self, text):\n",
        "        words = self.tokenize(text)\n",
        "\n",
        "        for word in words:\n",
        "            self.vocabulary.add(word)\n",
        "\n",
        "        padded = ['<START>'] * (self.n - 1) + words + ['<END>']\n",
        "\n",
        "        for i in range(len(padded) - self.n + 1):\n",
        "            ngram = tuple(padded[i:i + self.n])\n",
        "            context = ngram[:-1]\n",
        "\n",
        "            self.ngram_counts[ngram] = self.ngram_counts.get(ngram, 0) + 1\n",
        "            self.context_counts[context] = self.context_counts.get(context, 0) + 1\n",
        "\n",
        "    def probability(self, context, word):\n",
        "        if isinstance(context, list):\n",
        "            context = tuple(context)\n",
        "\n",
        "        ngram = context + (word,)\n",
        "        ngram_count = self.ngram_counts.get(ngram, 0)\n",
        "        context_count = self.context_counts.get(context, 0)\n",
        "\n",
        "        vocab_size = len(self.vocabulary)\n",
        "        return (ngram_count + 1) / (context_count + vocab_size)\n",
        "\n",
        "    def predict_next(self, context_words, top_k=5):\n",
        "        context = context_words[-(self.n-1):]\n",
        "        context = tuple(context)\n",
        "\n",
        "        predictions = []\n",
        "        for word in self.vocabulary:\n",
        "            prob = self.probability(context, word)\n",
        "            if prob > 0:\n",
        "                predictions.append((word, prob))\n",
        "\n",
        "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "        return predictions[:top_k]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('/content/train_twi.txt', 'r', encoding='utf-8') as f:\n",
        "        train_twi = f.read()\n",
        "\n",
        "    # Create and train model\n",
        "    model = NgramModel(n=2)  # bigram\n",
        "    model.train(train_twi)\n",
        "\n",
        "    print(f\"Vocabulary size: {len(model.vocabulary)}\")\n",
        "    print(f\"Unique bigrams: {len(model.ngram_counts)}\")\n",
        "\n",
        "    # Predict next word after \"me din\"\n",
        "    predictions = model.predict_next([\"me\", \"din\"], top_k=5)\n",
        "    print(\"\\nTop predictions after 'me din':\")\n",
        "    for word, prob in predictions:\n",
        "        print(f\"  {word}: {prob:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XDBH7GxjWwa"
      },
      "source": [
        "#Back Off Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JoTzNLGuOhTR"
      },
      "outputs": [],
      "source": [
        "class NgramModel:\n",
        "    def __init__(self, n=2):\n",
        "        self.n = n\n",
        "        self.ngram_counts = {}\n",
        "        self.context_counts = {}\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        import re\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^ɛɔ\\w\\s]', '', text)\n",
        "        words = text.split()\n",
        "        return words\n",
        "\n",
        "    def train(self, text):\n",
        "        words = self.tokenize(text)\n",
        "\n",
        "        for word in words:\n",
        "            self.vocabulary.add(word)\n",
        "\n",
        "        padded = ['<START>'] * (self.n - 1) + words + ['<END>']\n",
        "\n",
        "        for i in range(len(padded) - self.n + 1):\n",
        "            ngram = tuple(padded[i:i + self.n])\n",
        "            context = ngram[:-1]\n",
        "\n",
        "            self.ngram_counts[ngram] = self.ngram_counts.get(ngram, 0) + 1\n",
        "            self.context_counts[context] = self.context_counts.get(context, 0) + 1\n",
        "\n",
        "    def probability(self, context, word):\n",
        "        if isinstance(context, list):\n",
        "            context = tuple(context)\n",
        "\n",
        "        ngram = context + (word,)\n",
        "        ngram_count = self.ngram_counts.get(ngram, 0)\n",
        "        context_count = self.context_counts.get(context, 0)\n",
        "\n",
        "        # If we have seen this n-gram, use raw counts (no smoothing)\n",
        "        if ngram_count > 0 and context_count > 0:\n",
        "            return ngram_count / context_count\n",
        "\n",
        "        # If we haven't seen it, back off\n",
        "        elif len(context) > 0:\n",
        "            return 0.4 * self.probability(context[1:], word)\n",
        "        else:\n",
        "            return 1.0 / len(self.vocabulary)\n",
        "\n",
        "    def predict_next(self, context_words, top_k=5):\n",
        "        context = context_words[-(self.n-1):]\n",
        "        context = tuple(context)\n",
        "\n",
        "        predictions = []\n",
        "        for word in self.vocabulary:\n",
        "            prob = self.probability(context, word)\n",
        "            if prob > 0:\n",
        "                predictions.append((word, prob))\n",
        "\n",
        "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "        return predictions[:top_k]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open('/content/train_twi.txt', 'r', encoding='utf-8') as f:\n",
        "        twi_text = f.read()\n",
        "\n",
        "    model = NgramModel(n=2)\n",
        "    model.train(twi_text)\n",
        "\n",
        "    print(f\"Vocabulary size: {len(model.vocabulary)}\")\n",
        "    print(f\"Unique bigrams: {len(model.ngram_counts)}\")\n",
        "\n",
        "    # Predict next word after \"me din\"\n",
        "    predictions = model.predict_next([\"me\", \"din\"], top_k=5)\n",
        "    print(\"\\nTop predictions after 'me din':\")\n",
        "    for word, prob in predictions:\n",
        "        print(f\"  {word}: {prob:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8J63CIMjaaJ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "##Updated Backoff (where each ngram model holds its own counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anubiaaUQsQn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class NgramModelWithBackoff:\n",
        "    def __init__(self, n=3, alpha=0.4):\n",
        "        self.n = n\n",
        "        self.alpha = alpha  #backoff penalty\n",
        "        self.models = {} #storing models of different orders\n",
        "        for i in range(1, n + 1):\n",
        "            self.models[i] = {\n",
        "                'ngram_counts': {},\n",
        "                'context_counts': {},\n",
        "                'vocabulary': set()\n",
        "            }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        import re\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^ɛɔ\\w\\s]', '', text)\n",
        "        words = text.split()\n",
        "        return words\n",
        "\n",
        "    def train(self, text):\n",
        "        words = self.tokenize(text)\n",
        "\n",
        "        # training models of all orders\n",
        "        for order in range(1, self.n + 1):\n",
        "            vocab = self.models[order]['vocabulary']\n",
        "            vocab.add('<END>')\n",
        "            ngram_counts = self.models[order]['ngram_counts']\n",
        "            context_counts = self.models[order]['context_counts']\n",
        "\n",
        "            for word in words:\n",
        "                vocab.add(word)\n",
        "\n",
        "            padded = ['<START>'] * (order - 1) + words + ['<END>']\n",
        "\n",
        "            for i in range(len(padded) - order + 1):\n",
        "                ngram = tuple(padded[i:i + order])\n",
        "                context = ngram[:-1] if order > 1 else ()\n",
        "\n",
        "                ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
        "                if order > 1:\n",
        "                    context_counts[context] = context_counts.get(context, 0) + 1\n",
        "\n",
        "    def probability(self, context, word, order=None):\n",
        "        \"\"\"\n",
        "        Stupid backoff: if high-order n-gram not found, back off to lower order\n",
        "        \"\"\"\n",
        "        if order is None:\n",
        "            order = self.n\n",
        "\n",
        "        if isinstance(context, list):\n",
        "            context = tuple(context)\n",
        "\n",
        "        #only relevant context for this order\n",
        "        context = context[-(order-1):] if order > 1 else ()\n",
        "\n",
        "        model = self.models[order]\n",
        "        ngram = context + (word,)\n",
        "\n",
        "        ngram_count = model['ngram_counts'].get(ngram, 0)\n",
        "\n",
        "        if order == 1:\n",
        "            # Unigram: P(word) = count(word) / total_words\n",
        "            total_words = sum(model['ngram_counts'].values())\n",
        "            if total_words == 0:\n",
        "                return 1.0 / len(model['vocabulary']) if len(model['vocabulary']) > 0 else 1.0\n",
        "            return ngram_count / total_words if ngram_count > 0 else 1.0 / total_words\n",
        "\n",
        "        context_count = model['context_counts'].get(context, 0)\n",
        "\n",
        "        if ngram_count > 0 and context_count > 0:\n",
        "            return ngram_count / context_count\n",
        "        elif order > 1:\n",
        "            # Back off to shorter context with penalty\n",
        "            return self.alpha * self.probability(context[1:] if len(context) > 0 else (), word, order - 1)\n",
        "        else:\n",
        "            #fallback to unigram\n",
        "            return 1.0 / len(model['vocabulary']) if len(model['vocabulary']) > 0 else 1.0\n",
        "\n",
        "    def perplexity(self, test_text):\n",
        "        \"\"\"\n",
        "        Calculate perplexity on test text\n",
        "\n",
        "        Lower perplexity = better model\n",
        "        \"\"\"\n",
        "        words = self.tokenize(test_text)\n",
        "\n",
        "        if len(words) == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        #adding padding using <start>\n",
        "        padded = ['<START>'] * (self.n - 1) + words + ['<END>']\n",
        "\n",
        "        log_prob_sum = 0.0\n",
        "        word_count = 0\n",
        "\n",
        "        # Calculate probability for each word given its context\n",
        "        for i in range(self.n - 1, len(padded)):\n",
        "            context = tuple(padded[i - (self.n - 1):i])\n",
        "            word = padded[i]\n",
        "\n",
        "            prob = self.probability(context, word)\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log2(prob)\n",
        "                word_count += 1\n",
        "            else:\n",
        "                # If probability is 0, perplexity is infinite\n",
        "                return float('inf')\n",
        "\n",
        "        if word_count == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        avg_log_prob = log_prob_sum / word_count\n",
        "        perplexity = 2 ** (-avg_log_prob)\n",
        "\n",
        "        return perplexity\n",
        "\n",
        "    def predict_next(self, context_words, top_k=5):\n",
        "        \"\"\"Predict next word given context with top k approach\"\"\"\n",
        "        context = context_words[-(self.n-1):]\n",
        "        context = tuple(context)\n",
        "\n",
        "        predictions = []\n",
        "        vocab = self.models[self.n]['vocabulary']\n",
        "\n",
        "        for word in vocab:\n",
        "            prob = self.probability(context, word)\n",
        "            if prob > 0:\n",
        "                predictions.append((word, prob))\n",
        "\n",
        "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "        return predictions[:top_k]\n",
        "\n",
        "    def sample_next_word(self, context, temperature=1.0):\n",
        "      \"\"\"\n",
        "      Sample next word according to probability distribution (weighted sampling)\n",
        "\n",
        "      Args:\n",
        "          context: Previous words\n",
        "          temperature: Controls randomness (0.5=conservative, 1.0=normal, 2.0=creative)\n",
        "\n",
        "      Returns:\n",
        "          Sampled word\n",
        "      \"\"\"\n",
        "      import random\n",
        "      import math\n",
        "\n",
        "      context = tuple(context[-(self.n-1):])\n",
        "      vocab = self.models[self.n]['vocabulary']\n",
        "\n",
        "      # Get probabilities for all words\n",
        "      probs = []\n",
        "      words = []\n",
        "\n",
        "      for word in vocab:\n",
        "          prob = self.probability(context, word)\n",
        "          if prob > 0:\n",
        "              words.append(word)\n",
        "              probs.append(prob)\n",
        "\n",
        "      if len(words) == 0:\n",
        "          return '<END>'\n",
        "\n",
        "      # Apply temperature\n",
        "      if temperature != 1.0:\n",
        "          # Adjust probabilities: p_i = p_i^(1/T) / sum(p_j^(1/T))\n",
        "          adjusted_probs = [p ** (1.0 / temperature) for p in probs]\n",
        "          total = sum(adjusted_probs)\n",
        "          probs = [p / total for p in adjusted_probs]\n",
        "\n",
        "      # Sample weighted random choice\n",
        "      rand = random.random()\n",
        "      cumulative = 0.0\n",
        "\n",
        "      for word, prob in zip(words, probs):\n",
        "          cumulative += prob\n",
        "          if rand <= cumulative:\n",
        "              return word\n",
        "\n",
        "      return words[-1]\n",
        "\n",
        "    def sample_generate(self, start_context, max_length=20, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate sequence by sampling\n",
        "        \"\"\"\n",
        "        sequence = start_context.copy()\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            context = sequence[-(self.n-1):]\n",
        "            next_word = self.sample_next_word(context, temperature)\n",
        "\n",
        "            if next_word == '<END>':\n",
        "                break\n",
        "\n",
        "            sequence.append(next_word)\n",
        "\n",
        "        return sequence\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Training data\n",
        "    with open('/content/train_twi.txt', 'r', encoding='utf-8') as f:\n",
        "        train_text = f.read()\n",
        "\n",
        "    # Test data\n",
        "    with open('/content/test_twi.txt', 'r', encoding='utf-8') as f:\n",
        "        test_text = f.read()\n",
        "    # Val data\n",
        "    with open('/content/val_twi.txt', 'r', encoding='utf-8') as f:\n",
        "        val_text = f.read()\n",
        "\n",
        "    # Train models of different orders\n",
        "    print(\"Training models...\\n\")\n",
        "\n",
        "    bigram_model = NgramModelWithBackoff(n=2, alpha=0.4)\n",
        "    bigram_model.train(train_text)\n",
        "\n",
        "    trigram_model = NgramModelWithBackoff(n=3, alpha=0.4)\n",
        "    trigram_model.train(train_text)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    print(\"=\" * 50)\n",
        "    print(\"PERPLEXITY EVALUATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    bigram_perplexity = bigram_model.perplexity(test_text)\n",
        "    trigram_perplexity = trigram_model.perplexity(test_text)\n",
        "\n",
        "    print(f\"Bigram model perplexity:  {bigram_perplexity:.2f}\")\n",
        "    print(f\"Trigram model perplexity: {trigram_perplexity:.2f}\")\n",
        "\n",
        "    if bigram_perplexity < trigram_perplexity:\n",
        "        print(\"\\n Bigram model is better (lower perplexity)\")\n",
        "    else:\n",
        "        print(\"\\n Trigram model is better (lower perplexity)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"SAMPLE PREDICTIONS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nBigram predictions:\")\n",
        "    for i in range(3):\n",
        "        seq = bigram_model.sample_generate([\"me\", \"pɛ\"], temperature=0.8)\n",
        "        print(f\"Sample {i+1}: {' '.join(seq)}\")\n",
        "    print(\"\\nTrigram predictions:\")\n",
        "    for i in range(3):\n",
        "        seq = trigram_model.sample_generate([\"me\", \"pɛ\"], temperature=0.8)\n",
        "        print(f\"Sample {i+1}: {' '.join(seq)}\")\n",
        "\n",
        "# Using top k decoding\n",
        "    # test_context = [\"me\", \"pɛ\"]\n",
        "    # print(f\"\\nContext: {' '.join(test_context)}\")\n",
        "    # print(\"\\nBigram predictions:\")\n",
        "    # for word, prob in bigram_model.sample_generate(test_context, top_k=5):\n",
        "    #     print(f\"  {word}: {prob:.4f}\")\n",
        "\n",
        "    # print(\"\\nTrigram predictions:\")\n",
        "    # for word, prob in trigram_model.sample_generate(test_context, top_k=5):\n",
        "    #     print(f\"  {word}: {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa1HrsLDkR4Y"
      },
      "source": [
        "## Validation & Evaluation Experiments (with back off alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLGShWh9_RqJ"
      },
      "outputs": [],
      "source": [
        "def train_with_validation(model_class):\n",
        "    \"\"\"\n",
        "    Uses pre-split train/validation/test data from files.\n",
        "    \"\"\"\n",
        "    # Load pre-split data\n",
        "    with open('/content/train_twi.txt', 'r', encoding='utf-8') as f:\n",
        "        train_text = f.read()\n",
        "    with open('/content/val_twi.txt', 'r', encoding='utf-8') as f:\n",
        "        val_text = f.read()\n",
        "    with open('/content/test_twi.txt', 'r', encoding='utf-8') as f:\n",
        "        test_text = f.read()\n",
        "\n",
        "    # For reporting purposes, tokenize and count sentences for each set\n",
        "    # (using newline as a proxy for sentences in this context)\n",
        "    train_sentences = [s.strip() for s in train_text.split('\\n') if s.strip()]\n",
        "    val_sentences = [s.strip() for s in val_text.split('\\n') if s.strip()]\n",
        "    test_sentences = [s.strip() for s in test_text.split('\\n') if s.strip()]\n",
        "\n",
        "    print(f\"Train: {len(train_sentences)} sentences\")\n",
        "    print(f\"Validation: {len(val_sentences)} sentences\")\n",
        "    print(f\"Test: {len(test_sentences)} sentences\\n\")\n",
        "\n",
        "    # Try different hyperparameters\n",
        "    best_model = None\n",
        "    best_val_pp = float('inf')\n",
        "    results = []\n",
        "\n",
        "    for n_val in [2, 3, 4]:\n",
        "        for alpha in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "            model = model_class(n=n_val, alpha=alpha)\n",
        "            model.train(train_text)\n",
        "\n",
        "            val_pp = model.perplexity(val_text)\n",
        "            results.append((n_val, alpha, val_pp))\n",
        "\n",
        "            print(f\"n={n_val}, alpha={alpha:.1f}: validation perplexity={val_pp:.2f}\")\n",
        "\n",
        "            if val_pp < best_val_pp:\n",
        "                best_val_pp = val_pp\n",
        "                best_model = (n_val, alpha, model)\n",
        "\n",
        "    # Test best model\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Best model on validation set:\")\n",
        "    print(f\"  n={best_model[0]}, alpha={best_model[1]}\")\n",
        "    print(f\"  Validation perplexity: {best_val_pp:.2f}\")\n",
        "\n",
        "    test_pp = best_model[2].perplexity(test_text)\n",
        "    print(f\"  Test perplexity: {test_pp:.2f}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    return best_model, results\n",
        "\n",
        "# Usage\n",
        "best_model, results = train_with_validation(NgramModelWithBackoff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coVg2oAhnMly"
      },
      "source": [
        "#Analysing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtq56oLEnJSq"
      },
      "outputs": [],
      "source": [
        "def analyze_data_sparsity(model):\n",
        "    \"\"\"\n",
        "    Understand why predictions are uncertain\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"DATA SPARSITY ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for order in range(1, model.n + 1):\n",
        "        ngram_counts = model.models[order]['ngram_counts']\n",
        "        context_counts = model.models[order]['context_counts']\n",
        "        vocab = model.models[order]['vocabulary']\n",
        "\n",
        "        # Count how many n-grams appear only once\n",
        "        singletons = sum(1 for count in ngram_counts.values() if count == 1)\n",
        "        total_ngrams = len(ngram_counts)\n",
        "\n",
        "        # Calculate coverage\n",
        "        total_count = sum(ngram_counts.values())\n",
        "        singleton_mass = sum(count for count in ngram_counts.values() if count == 1)\n",
        "\n",
        "        print(f\"\\n{order}-grams:\")\n",
        "        print(f\"  Vocabulary size: {len(vocab)}\")\n",
        "        print(f\"  Unique {order}-grams: {total_ngrams}\")\n",
        "        print(f\"  Singletons (seen once): {singletons} ({100*singletons/total_ngrams:.1f}%)\")\n",
        "        print(f\"  Probability mass in singletons: {100*singleton_mass/total_count:.1f}%\")\n",
        "\n",
        "        if order == model.n:\n",
        "            # Check how many contexts have multiple continuations\n",
        "            multi_continuation = 0\n",
        "            for context in context_counts:\n",
        "                continuations = sum(1 for ng in ngram_counts if ng[:-1] == context)\n",
        "                if continuations > 1:\n",
        "                    multi_continuation += 1\n",
        "\n",
        "            print(f\"  Contexts with multiple continuations: {multi_continuation}/{len(context_counts)}\")\n",
        "\n",
        "analyze_data_sparsity(best_model[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjIYNf-mm7T_"
      },
      "source": [
        "#Testing Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPdK2L4Bkdtn"
      },
      "outputs": [],
      "source": [
        "class InterpolatedNgramModel:\n",
        "    def __init__(self, n=3):\n",
        "        self.n = n\n",
        "        self.models = {}\n",
        "        for i in range(1, n + 1):\n",
        "            self.models[i] = {\n",
        "                'ngram_counts': {},\n",
        "                'context_counts': {},\n",
        "                'vocabulary': set()\n",
        "            }\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        import re\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^ɛɔ\\w\\s]', '', text)\n",
        "        words = text.split()\n",
        "        return words\n",
        "\n",
        "    def train(self, text):\n",
        "        words = self.tokenize(text)\n",
        "\n",
        "        for order in range(1, self.n + 1):\n",
        "            vocab = self.models[order]['vocabulary']\n",
        "            ngram_counts = self.models[order]['ngram_counts']\n",
        "            context_counts = self.models[order]['context_counts']\n",
        "\n",
        "            for word in words:\n",
        "                vocab.add(word)\n",
        "\n",
        "            padded = ['<START>'] * (order - 1) + words + ['<END>']\n",
        "\n",
        "            for i in range(len(padded) - order + 1):\n",
        "                ngram = tuple(padded[i:i + order])\n",
        "                context = ngram[:-1] if order > 1 else ()\n",
        "\n",
        "                ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
        "                if order > 1:\n",
        "                    context_counts[context] = context_counts.get(context, 0) + 1\n",
        "\n",
        "    def probability(self, context, word, lambdas=None):\n",
        "        \"\"\"\n",
        "        Linear interpolation: mix all n-gram orders\n",
        "        \"\"\"\n",
        "        if lambdas is None:\n",
        "            lambdas = [0.5, 0.3, 0.2]  # Default for trigram\n",
        "\n",
        "        if isinstance(context, list):\n",
        "            context = tuple(context)\n",
        "\n",
        "        # Make sure lambdas sum to 1.0\n",
        "        lambda_sum = sum(lambdas)\n",
        "        lambdas = [l / lambda_sum for l in lambdas]\n",
        "\n",
        "        total_prob = 0.0\n",
        "\n",
        "        # Calculate probability from each order\n",
        "        for order in range(1, self.n + 1):\n",
        "            if order - 1 >= len(lambdas):\n",
        "                break\n",
        "\n",
        "            # Extract relevant context for this order\n",
        "            if order == 1:\n",
        "                relevant_context = ()\n",
        "            else:\n",
        "                # Take last (order-1) words from context\n",
        "                relevant_context = context[-(order-1):] if len(context) >= order - 1 else context\n",
        "\n",
        "            model = self.models[order]\n",
        "            ngram = relevant_context + (word,)\n",
        "\n",
        "            ngram_count = model['ngram_counts'].get(ngram, 0)\n",
        "\n",
        "            if order == 1:\n",
        "                # Unigram: P(word) = count(word) / total_words\n",
        "                total_words = sum(model['ngram_counts'].values())\n",
        "                if total_words > 0:\n",
        "                    prob = ngram_count / total_words\n",
        "                else:\n",
        "                    prob = 0\n",
        "            else:\n",
        "                # Higher order: P(word|context) = count(context,word) / count(context)\n",
        "                context_count = model['context_counts'].get(relevant_context, 0)\n",
        "                if context_count > 0:\n",
        "                    prob = ngram_count / context_count\n",
        "                else:\n",
        "                    prob = 0\n",
        "\n",
        "            # Add weighted probability\n",
        "            total_prob += lambdas[order - 1] * prob\n",
        "\n",
        "        if total_prob == 0:\n",
        "            return 1.0 / len(self.models[1]['vocabulary'])\n",
        "\n",
        "        return total_prob\n",
        "\n",
        "    def perplexity(self, test_text):\n",
        "        import math\n",
        "        words = self.tokenize(test_text)\n",
        "\n",
        "        if len(words) == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        padded = ['<START>'] * (self.n - 1) + words + ['<END>']\n",
        "\n",
        "        log_prob_sum = 0.0\n",
        "        word_count = 0\n",
        "\n",
        "        for i in range(self.n - 1, len(padded)):\n",
        "            context = tuple(padded[i - (self.n - 1):i])\n",
        "            word = padded[i]\n",
        "\n",
        "            prob = self.probability(context, word)\n",
        "\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log2(prob)\n",
        "                word_count += 1\n",
        "            else:\n",
        "                return float('inf')\n",
        "\n",
        "        if word_count == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        avg_log_prob = log_prob_sum / word_count\n",
        "        perplexity = 2 ** (-avg_log_prob)\n",
        "\n",
        "        return perplexity\n",
        "\n",
        "    def predict_next(self, context_words, top_k=5):\n",
        "        context = context_words[-(self.n-1):]\n",
        "        context = tuple(context)\n",
        "\n",
        "        predictions = []\n",
        "        vocab = self.models[self.n]['vocabulary']\n",
        "\n",
        "        for word in vocab:\n",
        "            prob = self.probability(context, word)\n",
        "            if prob > 0:\n",
        "                predictions.append((word, prob))\n",
        "\n",
        "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "        return predictions[:top_k]\n",
        "\n",
        "for lambdas in [[0.6, 0.3, 0.1], [0.5, 0.3, 0.2], [0.4, 0.4, 0.2]]:\n",
        "    model = InterpolatedNgramModel(n=3)\n",
        "    model.train(train_text)\n",
        "    val_pp = model.perplexity(val_text)\n",
        "    print(f\"lambdas={lambdas}: {val_pp:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}